{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490cb53b-3eb4-49ff-b101-241ee7de7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import norm\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"TkAgg\")\n",
    "import ddpm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17772673-bcac-4079-95cd-a2ae435e7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = range(100, 2900, 400)\n",
    "sizes = range(3300, 5000, 400)\n",
    "names = [f\"square_{s}\" for s in sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a886df7-a87f-46a4-a911-c5bcf40cd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "Training model 0...\n",
      "END LOSS WAS 0.7883929517014866\n",
      "Saving model...\n",
      "Training model 1...\n",
      "END LOSS WAS 0.8827568950630185\n",
      "Saving model...\n",
      "Training model 2...\n",
      "END LOSS WAS 0.8948706064644449\n",
      "Saving model...\n",
      "Training model 3...\n",
      "END LOSS WAS 0.9953284821443878\n",
      "Saving model...\n",
      "Training model 4...\n",
      "END LOSS WAS 0.8387160900933767\n",
      "Saving model...\n",
      "Training model 5...\n",
      "END LOSS WAS 0.792220150172525\n",
      "Saving model...\n",
      "Training model 6...\n",
      "END LOSS WAS 0.6525430478538816\n",
      "Saving model...\n",
      "Training model 7...\n",
      "END LOSS WAS 0.846561387179412\n",
      "Saving model...\n",
      "Training model 8...\n",
      "END LOSS WAS 0.944341589561114\n",
      "Saving model...\n",
      "Training model 9...\n",
      "END LOSS WAS 0.89935436081184\n",
      "Saving model...\n",
      "Training model 10...\n",
      "END LOSS WAS 0.8581838666633628\n",
      "Saving model...\n",
      "Training model 11...\n",
      "END LOSS WAS 0.9227606092040953\n",
      "Saving model...\n",
      "Training model 12...\n",
      "END LOSS WAS 0.8545517535584487\n",
      "Saving model...\n",
      "Training model 13...\n",
      "END LOSS WAS 0.7768750818378969\n",
      "Saving model...\n",
      "Training model 14...\n",
      "END LOSS WAS 0.8103196317036163\n",
      "Saving model...\n",
      "Training model 15...\n",
      "END LOSS WAS 0.49085957440147565\n",
      "Saving model...\n",
      "Training model 16...\n",
      "END LOSS WAS 0.5729190662307029\n",
      "Saving model...\n",
      "Training model 17...\n",
      "END LOSS WAS 0.3322253046760141\n",
      "Saving model...\n",
      "Training model 18...\n",
      "END LOSS WAS 0.7510100965474562\n",
      "Saving model...\n",
      "Training model 19...\n",
      "END LOSS WAS 0.525439106912542\n",
      "Saving model...\n",
      "Training model 20...\n",
      "END LOSS WAS 0.42136708698896486\n",
      "Saving model...\n",
      "Training model 21...\n",
      "END LOSS WAS 0.47627510102096327\n",
      "Saving model...\n",
      "Training model 22...\n",
      "END LOSS WAS 0.5761460521305879\n",
      "Saving model...\n",
      "Training model 23...\n",
      "END LOSS WAS 0.42327272514143827\n",
      "Saving model...\n",
      "Training model 24...\n",
      "END LOSS WAS 0.5973427961982459\n",
      "Saving model...\n",
      "Training model 25...\n",
      "END LOSS WAS 0.4026083061134068\n",
      "Saving model...\n",
      "Training model 26...\n",
      "END LOSS WAS 0.5264233738078079\n",
      "Saving model...\n",
      "Training model 27...\n",
      "END LOSS WAS 0.4586404598105608\n",
      "Saving model...\n",
      "Training model 28...\n",
      "END LOSS WAS 0.26928450878908206\n",
      "Saving model...\n",
      "Training model 29...\n",
      "END LOSS WAS 0.39824837131377\n",
      "Saving model...\n",
      "Training model 30...\n",
      "END LOSS WAS 0.7352355293211434\n",
      "Saving model...\n",
      "Training model 31...\n",
      "END LOSS WAS 0.512664750173044\n",
      "Saving model...\n",
      "Training model 32...\n",
      "END LOSS WAS 0.5736079398585161\n",
      "Saving model...\n",
      "Training model 33...\n",
      "END LOSS WAS 0.7169997079236213\n",
      "Saving model...\n",
      "Training model 34...\n",
      "END LOSS WAS 0.2943444599710108\n",
      "Saving model...\n",
      "Training model 35...\n",
      "END LOSS WAS 0.30624141894938484\n",
      "Saving model...\n",
      "Training model 36...\n",
      "END LOSS WAS 0.2173705058148421\n",
      "Saving model...\n",
      "Training model 37...\n",
      "END LOSS WAS 0.19997531569534313\n",
      "Saving model...\n",
      "Training model 38...\n",
      "END LOSS WAS 0.08772589373735004\n",
      "Saving model...\n",
      "Training model 39...\n",
      "END LOSS WAS 0.053818814337670356\n",
      "Saving model...\n",
      "Training model 40...\n",
      "END LOSS WAS 0.03488111359687273\n",
      "Saving model...\n",
      "Training model 41...\n",
      "END LOSS WAS 0.01859991999979401\n",
      "Saving model...\n",
      "Training model 42...\n",
      "END LOSS WAS 0.015062103469700635\n",
      "Saving model...\n",
      "Training model 43...\n",
      "END LOSS WAS 0.008666811250865804\n",
      "Saving model...\n",
      "Training model 44...\n",
      "END LOSS WAS 0.0038540705742832336\n",
      "Saving model...\n",
      "Training model 45...\n",
      "END LOSS WAS 0.002388016351406459\n",
      "Saving model...\n",
      "Training model 46...\n",
      "END LOSS WAS 0.0019880478712755974\n",
      "Saving model...\n",
      "Training model 47...\n",
      "END LOSS WAS 0.0011874842598443647\n",
      "Saving model...\n",
      "Training model 48...\n",
      "END LOSS WAS 0.001929114809851674\n",
      "Saving model...\n",
      "Training model 49...\n",
      "END LOSS WAS 0.0007993921937458152\n",
      "Saving model...\n",
      "Saving loss as numpy array...\n",
      "3700\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "Training model 0...\n",
      "END LOSS WAS 0.9109515720244841\n",
      "Saving model...\n",
      "Training model 1...\n",
      "END LOSS WAS 0.7734371591191228\n",
      "Saving model...\n",
      "Training model 2...\n",
      "END LOSS WAS 0.9891571133253105\n",
      "Saving model...\n",
      "Training model 3...\n",
      "END LOSS WAS 1.4196378125630114\n",
      "Saving model...\n",
      "Training model 4...\n",
      "END LOSS WAS 0.9401721654685387\n",
      "Saving model...\n",
      "Training model 5...\n",
      "END LOSS WAS 0.9744526292453948\n",
      "Saving model...\n",
      "Training model 6...\n",
      "END LOSS WAS 0.8471304435996225\n",
      "Saving model...\n",
      "Training model 7...\n",
      "END LOSS WAS 1.0271546877668198\n",
      "Saving model...\n",
      "Training model 8...\n",
      "END LOSS WAS 1.0799100157460042\n",
      "Saving model...\n",
      "Training model 9...\n",
      "END LOSS WAS 0.9320636829417818\n",
      "Saving model...\n",
      "Training model 10...\n",
      "END LOSS WAS 1.3189959182352131\n",
      "Saving model...\n",
      "Training model 11...\n",
      "END LOSS WAS 0.6716445046900278\n",
      "Saving model...\n",
      "Training model 12...\n",
      "END LOSS WAS 1.002787472868781\n",
      "Saving model...\n",
      "Training model 13...\n",
      "END LOSS WAS 0.7983733432207818\n",
      "Saving model...\n",
      "Training model 14...\n",
      "END LOSS WAS 0.5963863582303296\n",
      "Saving model...\n",
      "Training model 15...\n",
      "END LOSS WAS 0.652728479321099\n",
      "Saving model...\n",
      "Training model 16...\n",
      "END LOSS WAS 0.32086939963343897\n",
      "Saving model...\n",
      "Training model 17...\n",
      "END LOSS WAS 0.4893747548061417\n",
      "Saving model...\n",
      "Training model 18...\n",
      "END LOSS WAS 0.6566871334592865\n",
      "Saving model...\n",
      "Training model 19...\n",
      "END LOSS WAS 0.5641496781449986\n",
      "Saving model...\n",
      "Training model 20...\n",
      "END LOSS WAS 0.327982343401813\n",
      "Saving model...\n",
      "Training model 21...\n",
      "END LOSS WAS 0.4547733344983659\n",
      "Saving model...\n",
      "Training model 22...\n",
      "END LOSS WAS 0.47456429496772706\n",
      "Saving model...\n",
      "Training model 23...\n",
      "END LOSS WAS 0.4600900010323981\n",
      "Saving model...\n",
      "Training model 24...\n",
      "END LOSS WAS 0.4188592067523839\n",
      "Saving model...\n",
      "Training model 25...\n",
      "END LOSS WAS 0.4326955714780192\n",
      "Saving model...\n",
      "Training model 26...\n",
      "END LOSS WAS 0.6945873622276352\n",
      "Saving model...\n",
      "Training model 27...\n",
      "END LOSS WAS 0.5190411904540467\n",
      "Saving model...\n",
      "Training model 28...\n",
      "END LOSS WAS 0.658965764523926\n",
      "Saving model...\n",
      "Training model 29...\n",
      "END LOSS WAS 0.48661005074426883\n",
      "Saving model...\n",
      "Training model 30...\n",
      "END LOSS WAS 0.5251579759775378\n",
      "Saving model...\n",
      "Training model 31...\n",
      "END LOSS WAS 0.6758936047572704\n",
      "Saving model...\n",
      "Training model 32...\n",
      "END LOSS WAS 0.5024288865477963\n",
      "Saving model...\n",
      "Training model 33...\n",
      "END LOSS WAS 0.5090548025014078\n",
      "Saving model...\n",
      "Training model 34...\n",
      "END LOSS WAS 0.5253343071355419\n",
      "Saving model...\n",
      "Training model 35...\n",
      "END LOSS WAS 0.31619815624239606\n",
      "Saving model...\n",
      "Training model 36...\n",
      "END LOSS WAS 0.24682208127926403\n",
      "Saving model...\n",
      "Training model 37...\n",
      "END LOSS WAS 0.16211004262107734\n",
      "Saving model...\n",
      "Training model 38...\n",
      "END LOSS WAS 0.09636213923585761\n",
      "Saving model...\n",
      "Training model 39...\n",
      "END LOSS WAS 0.06655301270310784\n",
      "Saving model...\n",
      "Training model 40...\n",
      "END LOSS WAS 0.03142367385778596\n",
      "Saving model...\n",
      "Training model 41...\n",
      "END LOSS WAS 0.02129763906091878\n",
      "Saving model...\n",
      "Training model 42...\n",
      "END LOSS WAS 0.011716019935252536\n",
      "Saving model...\n",
      "Training model 43...\n",
      "END LOSS WAS 0.008161132392990085\n",
      "Saving model...\n",
      "Training model 44...\n",
      "END LOSS WAS 0.003946703879536763\n",
      "Saving model...\n",
      "Training model 45...\n",
      "END LOSS WAS 0.002927603090801743\n",
      "Saving model...\n",
      "Training model 46...\n",
      "END LOSS WAS 0.001801229052538007\n",
      "Saving model...\n",
      "Training model 47...\n",
      "END LOSS WAS 0.0015017777821045328\n",
      "Saving model...\n",
      "Training model 48...\n",
      "END LOSS WAS 0.0009678903304450831\n",
      "Saving model...\n",
      "Training model 49...\n",
      "END LOSS WAS 0.0005361019819886304\n",
      "Saving model...\n",
      "Saving loss as numpy array...\n",
      "4100\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "Training model 0...\n",
      "END LOSS WAS 1.1270167347711646\n",
      "Saving model...\n",
      "Training model 1...\n",
      "END LOSS WAS 1.0735035664833568\n",
      "Saving model...\n",
      "Training model 2...\n",
      "END LOSS WAS 1.0613699297832409\n",
      "Saving model...\n",
      "Training model 3...\n",
      "END LOSS WAS 0.9536900254332626\n",
      "Saving model...\n",
      "Training model 4...\n",
      "END LOSS WAS 0.9260105443606746\n",
      "Saving model...\n",
      "Training model 5...\n",
      "END LOSS WAS 1.1922647465506702\n",
      "Saving model...\n",
      "Training model 6...\n",
      "END LOSS WAS 1.0252904982509246\n",
      "Saving model...\n",
      "Training model 7...\n",
      "END LOSS WAS 0.9434663969020808\n",
      "Saving model...\n",
      "Training model 8...\n",
      "END LOSS WAS 0.6537050794454387\n",
      "Saving model...\n",
      "Training model 9...\n",
      "END LOSS WAS 0.8511677445857233\n",
      "Saving model...\n",
      "Training model 10...\n",
      "END LOSS WAS 1.0113936362057077\n",
      "Saving model...\n",
      "Training model 11...\n",
      "END LOSS WAS 0.9641510521366232\n",
      "Saving model...\n",
      "Training model 12...\n",
      "END LOSS WAS 1.0965466201839331\n",
      "Saving model...\n",
      "Training model 13...\n",
      "END LOSS WAS 0.6417548569490903\n",
      "Saving model...\n",
      "Training model 14...\n",
      "END LOSS WAS 0.6715231962085537\n",
      "Saving model...\n",
      "Training model 15...\n",
      "END LOSS WAS 0.4811083350677359\n",
      "Saving model...\n",
      "Training model 16...\n",
      "END LOSS WAS 0.6684566632517494\n",
      "Saving model...\n",
      "Training model 17...\n",
      "END LOSS WAS 0.5807563092510593\n",
      "Saving model...\n",
      "Training model 18...\n",
      "END LOSS WAS 0.6088180013211009\n",
      "Saving model...\n",
      "Training model 19...\n",
      "END LOSS WAS 0.6076640898979322\n",
      "Saving model...\n",
      "Training model 20...\n",
      "END LOSS WAS 0.4343413634466925\n",
      "Saving model...\n",
      "Training model 21...\n",
      "END LOSS WAS 0.5827069297839117\n",
      "Saving model...\n",
      "Training model 22...\n",
      "END LOSS WAS 0.5018972503809986\n",
      "Saving model...\n",
      "Training model 23...\n",
      "END LOSS WAS 0.3943505766075982\n",
      "Saving model...\n",
      "Training model 24...\n",
      "END LOSS WAS 0.608083437504239\n",
      "Saving model...\n",
      "Training model 25...\n",
      "END LOSS WAS 0.5185937639418505\n",
      "Saving model...\n",
      "Training model 26...\n",
      "END LOSS WAS 0.6063857606050385\n",
      "Saving model...\n",
      "Training model 27...\n",
      "END LOSS WAS 0.4438952703365868\n",
      "Saving model...\n",
      "Training model 28...\n",
      "END LOSS WAS 0.45725212081912003\n",
      "Saving model...\n",
      "Training model 29...\n",
      "END LOSS WAS 0.38076522893012255\n",
      "Saving model...\n",
      "Training model 30...\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sizes)):\n",
    "    print(sizes[i])\n",
    "    !python ddpm.py --dataset square --experiment_name {names[i]} --num_epochs 50 --dataset_size {sizes[i]} --beta_schedule ours --num_timesteps 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074d1abf-9bd3-4f20-90f5-804fe97345a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(model_dir, dataset='point', score='model'):\n",
    "    num_scheduler_timesteps = 1000\n",
    "    noise_scheduler = ddpm.NoiseScheduler(num_timesteps=num_scheduler_timesteps, beta_schedule='ours')\n",
    "    num_timesteps = len(noise_scheduler)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    models = [ddpm.MLP(input_dim=2) for _ in range(num_timesteps)]\n",
    "    for t in range(num_timesteps):\n",
    "        path = model_dir + f\"/model{t}.pth\"\n",
    "        models[t].load_state_dict(torch.load(path))\n",
    "        models[t].to(device)\n",
    "        models[t].eval()\n",
    "    eval_batch_size = 100000\n",
    "    plot_step = 1\n",
    "    num_timesteps = len(noise_scheduler.betas)\n",
    "    curr_vars = torch.sqrt(1 - torch.exp(-2 * noise_scheduler.times))\n",
    "    sample = torch.randn(eval_batch_size, 2).to(device).to(device)\n",
    "    timesteps = list(range(num_timesteps))[::-1][:-5]\n",
    "    print(\"last std\", curr_vars[timesteps[-1]])\n",
    "    samples = []\n",
    "    steps = []\n",
    "    for i, t in enumerate(tqdm(timesteps)):\n",
    "        t_int = t\n",
    "        t = torch.from_numpy(np.repeat(t, eval_batch_size)).long().to(device)\n",
    "        with torch.no_grad():\n",
    "            variance = torch.sqrt(1 - noise_scheduler.alphas_cumprod[t])\n",
    "            v = curr_vars[t].cpu().numpy()\n",
    "            if score == 'model':\n",
    "                residual = models[t_int](sample, t)\n",
    "            else:\n",
    "                residual = sample / variance\n",
    "        sample = noise_scheduler.step(residual, t[0], sample)\n",
    "        if (i + 1) % plot_step == 0:\n",
    "            sample_cpu = sample.cpu()\n",
    "            samples.append(sample_cpu.numpy())\n",
    "            steps.append(i + 1)\n",
    "    print(samples[-1])\n",
    "    return process_square(samples[-1], mode='median')\n",
    "def process_square(samples, r=3, mode='median'):\n",
    "    t = 0\n",
    "    d = []\n",
    "    for s in samples:\n",
    "        dists = [abs(s[0] - r), abs(s[0] + r), abs(s[1] - r), abs(s[1] + r)]\n",
    "        m = min(dists)\n",
    "        d.append(m)\n",
    "    # d = sorted(d)[:-round(len(d)*0.02)]\n",
    "    if mode == 'median':\n",
    "        return sorted(d)[len(d)//2]\n",
    "    for m in d:\n",
    "        t += m**2\n",
    "    t /= len(d)\n",
    "    return np.sqrt(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a92977cc-20b5-4679-ac33-c41820621ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 square_100\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039db289a01d47e1b58ac7f33745aa55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.64826575  2.99872772]\n",
      " [-2.99006477 -0.50314895]\n",
      " [ 1.77686995  1.84478933]\n",
      " ...\n",
      " [ 1.77888957  2.8973803 ]\n",
      " [ 2.99471784  2.21419741]\n",
      " [-2.99072528 -0.72244898]]\n",
      "500 square_500\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ebef57ee9641669ed538fe23dadb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.00376705 -1.4585616 ]\n",
      " [ 0.10081579 -2.99407349]\n",
      " [-2.99109324 -1.59602885]\n",
      " ...\n",
      " [ 2.99425463  2.85610399]\n",
      " [-2.99501285  0.95785781]\n",
      " [ 0.61249264 -3.00406183]]\n",
      "900 square_900\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9d6785b98941faad9bbbe9568d1130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00473842  2.99920833]\n",
      " [ 2.46445873 -3.00317763]\n",
      " [-2.99467551 -1.62423302]\n",
      " ...\n",
      " [-2.99871169  2.12725218]\n",
      " [-2.97971666 -3.00111218]\n",
      " [ 0.31233414  3.00184258]]\n",
      "1300 square_1300\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813476a64cf5453aa018d8899676f1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.57826139 -2.9931353 ]\n",
      " [-3.00098973  1.12282135]\n",
      " [-3.00228268 -1.61155334]\n",
      " ...\n",
      " [-1.93149914  3.00033266]\n",
      " [-3.0005271   2.72631702]\n",
      " [-2.99934823  1.95536906]]\n",
      "1700 square_1700\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fe836c3e924ac3854e1549c0187bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.57293442 -2.99952751]\n",
      " [-0.41646793 -3.00097617]\n",
      " [-2.33944994  2.99829267]\n",
      " ...\n",
      " [-3.00077073  0.02159166]\n",
      " [ 0.64416453  2.99766105]\n",
      " [ 1.04463829 -2.99766895]]\n",
      "2100 square_2100\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516abffe05304fb5a1ba438d5168c8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.00068083  0.28152951]\n",
      " [ 0.89915467 -3.00166531]\n",
      " [ 2.99842278 -2.45978814]\n",
      " ...\n",
      " [-3.00048949 -0.10187411]\n",
      " [-2.86839094  3.00058721]\n",
      " [ 2.99763768 -2.33593283]]\n",
      "2500 square_2500\n",
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n",
      "last std tensor(4.2208e-05, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dd6d8bb2be4870b5298dd6a02a6298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.99966079  0.7490067 ]\n",
      " [-2.99909234  2.02180291]\n",
      " [ 3.00062481 -1.55088826]\n",
      " ...\n",
      " [ 2.99807289  1.38332462]\n",
      " [ 3.00103032 -1.03280146]\n",
      " [ 0.79312873 -3.00109854]]\n",
      "[0.010505383737573482, 0.003032553392020798, 0.0019632330676753185, 0.0018738624552425698, 0.0013212902354555744, 0.0013010420442616244, 0.0012681355097159397]\n"
     ]
    }
   ],
   "source": [
    "s = []\n",
    "for i in range(len(sizes)):\n",
    "    print(sizes[i], names[i])\n",
    "    s.append(np.abs(calculate_stats(f\"exps/{names[i]}\", dataset='square', score='model')))\n",
    "print(s)\n",
    "plt.clf()\n",
    "plt.scatter(sizes, np.array(s))\n",
    "plt.yscale('log')\n",
    "plt.title(\"median error vs dataset size\")\n",
    "plt.ylabel(\"median error\")\n",
    "plt.xlabel(\"dataset size\")\n",
    "plt.savefig(f'static/devs_square.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c9aec51-0182-40f6-af6a-518dbed64a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMIN GAMMA tensor(4.3165e-06, dtype=torch.float64) LEN 50\n"
     ]
    }
   ],
   "source": [
    "model_dir = f\"exps/{names[-1]}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "noise_scheduler = ddpm.NoiseScheduler(num_timesteps=1000, beta_schedule='ours')\n",
    "num_timesteps = len(noise_scheduler.times)\n",
    "models = [ddpm.MLP(input_dim=1) for _ in range(num_timesteps)]\n",
    "for t in range(num_timesteps):\n",
    "    path = model_dir + f\"/model{t}.pth\"\n",
    "    models[t].load_state_dict(torch.load(path))\n",
    "    models[t].to(device)\n",
    "    models[t].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19966cfd-1f5a-4ae6-9e6d-e980a6a3e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003080510901544058\n"
     ]
    }
   ],
   "source": [
    "curr_stds = torch.sqrt(1 - torch.exp(-2 * noise_scheduler.times))\n",
    "t = 10\n",
    "v = curr_stds[t].item()\n",
    "print(v)\n",
    "x_scale = np.linspace(-v * 5, v * 5, 1000)\n",
    "# x_scale = np.linspace(-10, 10, 1000)\n",
    "inputs = torch.tensor(x_scale, device=device).unsqueeze(1)\n",
    "times = torch.ones(len(inputs)).to(device) * t\n",
    "model_residuals = models[t](inputs, times)\n",
    "true_residuals = inputs / v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9084b4e-6f81-478c-ab30-8c05d17438c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_scale, model_residuals.data.cpu().numpy(), label='model')\n",
    "plt.plot(x_scale, true_residuals.data.cpu().numpy(), label='true')\n",
    "for y in (np.arange(11)-5)*v:\n",
    "    plt.axvline(y, alpha=1 if y == 0 else 0.2)\n",
    "plt.legend()\n",
    "plt.savefig(\"score.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07e3bc9-fe05-44ba-9378-a636c48ed259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'curr_vars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t)\n\u001b[0;32m----> 4\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_vars\u001b[49m[t]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m     x_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39mv\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m, v\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      6\u001b[0m     diff \u001b[38;5;241m=\u001b[39m x_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m x_range[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_vars' is not defined"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for t in range(50):\n",
    "    print(t)\n",
    "    v = curr_vars[t].cpu().numpy()\n",
    "    x_range = np.linspace(-v*5, v*5, 1000)\n",
    "    diff = x_range[1] - x_range[0]\n",
    "    l2 = 0\n",
    "    difc = 0\n",
    "    pc = 0\n",
    "    tot = 0\n",
    "    for i in x_range:\n",
    "        v = curr_vars[t]\n",
    "        pdf = norm.pdf(i, 0, v.item())\n",
    "        model_val = model(torch.tensor([[i]], device=device, dtype=torch.float32), torch.ones(1, device=device, dtype=torch.float32)*t)\n",
    "        true_val = torch.tensor([[i]], device=device) / torch.sqrt(1 - torch.exp(-2 * noise_scheduler.times[t]))\n",
    "        error = (model_val.data.cpu().numpy() - true_val.data.cpu().numpy())[0]\n",
    "        l2 += (error**2)*diff*pdf\n",
    "        difc += diff\n",
    "        pc += pdf\n",
    "        tot += diff*pdf\n",
    "    print(l2, difc, tot, v.item()*2)\n",
    "    errors.append(l2*v.item()*v.item())\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc504836-98f0-4510-9905-3ee2d4bcc2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6831737563750486\n"
     ]
    }
   ],
   "source": [
    "x_range = np.linspace(-10, 10, 1000)\n",
    "diff = x_range[1] - x_range[0]\n",
    "tot = 0\n",
    "for i in x_range:\n",
    "    tot += diff * norm.pdf(i, 0, 10)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c7a860e-5ded-47d4-9f24-af54828102c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN GAMMA tensor(3.1610e-06, dtype=torch.float64) LEN 84\n",
      "torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "model_path = \"exps/point_1d_smallgamma7300/model.pth\"\n",
    "model = ddpm.MLP(input_dim=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "scores = []\n",
    "inputs = torch.linspace(-10, 10, 1000).unsqueeze(1).to(device)\n",
    "noise_scheduler = ddpm.NoiseScheduler(num_timesteps=1000, beta_schedule=\"ours\")\n",
    "print(noise_scheduler.betas.shape)\n",
    "times = torch.ones(len(inputs), device=device)*9\n",
    "residuals = model(inputs, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a7a4d-2e41-45ee-afa2-932cdd37dac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
